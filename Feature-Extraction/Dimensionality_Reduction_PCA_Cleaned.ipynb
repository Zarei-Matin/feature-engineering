{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53189d7e",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction using PCA\n",
    "### Principal Component Analysis explained with code and plots\n",
    "\n",
    "In this notebook, we explore how Principal Component Analysis (PCA) can be used to reduce the dimensionality of datasets while retaining as much variability as possible. This includes step-by-step implementation with explanations and visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8e9a02-44ba-42bf-9226-0dd2a41aff8d",
   "metadata": {},
   "source": [
    "![توضیح عکس](images/122a1660-5597-4287-8c40-9399fccd6554.png)\n",
    "<div style=\"direction: rtl; line-height: 300%;\">\n",
    "    <font face=\"XB Zar\" size=\"5\">\n",
    "        <div align=\"center\">\n",
    "            <font face=\"IranNastaliq\" size=\"30\">\n",
    "                <p></p>\n",
    "                <p></p>\n",
    "                    به نام آنکه جان را فکرت آموخت\n",
    "                <p></p>\n",
    "            </font>\n",
    "            <img src=\"images/logo.png\" alt=\"توضیح عکس\" width=\"200\"> <br>\n",
    "            <font color=\"#222831\">\n",
    "                 دانشکده علوم و فنون بین رشته ای\n",
    "            </font>\n",
    "            <p></p><br>\n",
    "            <span style=\"color: #27548A; font-family: BTitr; font-size: 32px; font-weight: bold;\">\n",
    "                شناسایی الگو\n",
    "            </span>\n",
    "            <br><br>\n",
    "                <span style=\"font-size: 18px;\">اردیبهشت 1404</span>\n",
    "        </div>\n",
    "        <hr>\n",
    "        <font color=\"#E55050\" size=\"6\">\n",
    "            <br>\n",
    "            <div align=\"center\">\n",
    "                <span style=\"font-family: B Titr; font-size: 32px;\">استخراج ویژگی - کاهش بعد - PCA</span>\n",
    "            </div>\n",
    "        </font>\n",
    "        <br>\n",
    "        <div align=\"center\">\n",
    "            متین زارعی - دکتر حمیدرضا افتخاری\n",
    "        </div>\n",
    "        <hr>\n",
    "        <div style=\"direction: rtl\">\n",
    "            <h3>فهرست مطالب</h3>\n",
    "            <ul>\n",
    "                <li>مقدمه</li>\n",
    "                <li>شهود مساله</li>\n",
    "                <li>تعریف نماد های ریاضی </li>\n",
    "                <li>نگاشت خطی</li>\n",
    "                <li>واریانس و واریانس تجربی</li>\n",
    "                <li>مولفه های اصلی</li>\n",
    "                <li>نگاشت ویژگی ها</li>\n",
    "                <li>پیاده سازی PCA</li>\n",
    "                <li>سفید سازی و ZCA</li>\n",
    "                <li>ملاحظات و محدودیت های PCA</li>\n",
    "                <li>کاربرد ها</li>\n",
    "                <li>خلاصه</li>\n",
    "                <li>منابع</li>\n",
    "            </ul>\n",
    "        </div>\n",
    "        <br><br>\n",
    "        <div>\n",
    "            <h3>مقدمه</h3>\n",
    "            <hr>\n",
    "            <p style=\"text-align: justify;\">\n",
    "                با استفاده از روش های خودکار جمع آوری داده و تولید ویژگی، می توان به سرعت به تعداد زیادی ویژگی دست یافت. اما همه این ویژگی ها مفید نیستند. در فصل های قبلی درباره روش هایی مانند \n",
    "                پالایش مبتنی بر فراوانی و مقیاس گذاری ویژگی ها صحبت کردیم. اکنون قصد داریم به طور دقیق به موضوع کاهش ابعاد ویژگی ها با استفاده از تحلیل مولفه های اصلی (PCA) بپردازیم.\n",
    "                تا پیش از این تکنیک های معرفی شده بدون نیاز به رجوع به داده قابل تعریف بودند. برای مثال، فیلتر کردن بر اساس فراوانی ممکن است اینگونه تعریف شود \"تمام شمارش هایی که کمتر از n\n",
    "                هستند را حذف کن\" این رویه بدون نیاز به دریافت اطلاعات بیشتر از داده ها قابل انجام است.\n",
    "            </p>\n",
    "            <hr><br>\n",
    "        </div>\n",
    "        <div>\n",
    "            <h3>شهود مساله</h3>\n",
    "            <hr>\n",
    "            <p style=\"text-align: justify;\">\n",
    "                کاهش ابعاد به معنای حذف «اطلاعات بی‌فایده» و در عین حال حفظ بخش‌های حیاتی اطلاعات است. روش‌های مختلفی برای تعریف «بی‌فایده بودن» وجود دارد. تحلیل مؤلفه‌های اصلی (PCA) بر مفهوم وابستگی خطی تمرکز می‌کند. در ماتریس ها ما فضای ستونی یک ماتریس داده را به‌عنوان گستره‌ای از تمامی بردارهای ویژگی توصیف میکنیم. اگر فضای ستونی کوچک‌تر از تعداد کل ویژگی‌ها باشد، به این معناست که بیشتر ویژگی‌ها ترکیبی خطی از تعداد کمی ویژگی کلیدی هستند.\n",
    "ویژگی‌هایی که به‌صورت خطی به یکدیگر وابسته‌اند، اتلاف فضا و توان محاسباتی به‌شمار می‌آیند، زیرا اطلاعات آن‌ها می‌توانست با تعداد بسیار کمتری از ویژگی‌ها کدگذاری شود. به‌منظور اجتناب از این وضعیت، تحلیل مؤلفه‌های اصلی (PCA) تلاش می‌کند چنین «حشو»ی را کاهش دهد، با فشرده‌سازی داده‌ها در یک زیرفضای خطی با ابعاد بسیار پایین‌تر.\n",
    "     تصور کنید که مجموعه ای از نقاط داده در فضای ویژگی ها قرار دارد. هر نقطه ی داده یک نقطه (dot) است و کل مجموعه ی نقاط داده یک توده (blob) را تشکیل میدهند.\n",
    "                درشکل 6-1 (a) نقاط داده به طور یکنواخت در هر دو بعد ویژگی پخش شده اند و توده فضای موجود را پر کرده است. در این مثال فضای ستونی دارای رتبه کامل (full rank) است.\n",
    "                اما اگر برخی از این ویژگی ها ترکیبات خطی از سایر ویژگی ها باشند، آنگاه این توده دیگر اینگونه پر حجم به نظر نمیرسد بلکه شبیه شکل 6-1 (b) خواهد بود، یعنی یک توده تخت\n",
    "                که ویژگی 2 تکراری (یا ضریبی) از ویژگی 1 است. درچنین حالتی میگوییم بعد ذاتی توده برابر 1 است، هرچند که توده در فضای ویژگی 2 بعدی قرار دارد .\n",
    "                در عمل موارد نادری وجود دارد که دو ویژگی کاملا برابر باشند. بیشتر اوقات ویژگی هایی را می بینیم که نقریبا برابرند، اما نه دقیقا. در چنین حالتی، توده ی داده ممکن است شبیه شکل 6-1 (c) \n",
    "                باشد. یک توده لاغر و کشیده. اگر بخواهیم ویژگی هایی که به مدل داده میشود را کاهش بدهیم می توانیم ویژگی 1 و ویژگی 2 را با یک ویژگی جدید جایگزین کنیم. مثلا به اسم ویژگی 1.5 که این\n",
    "                ویژگی روی خط قطری بین دو ویژگی اصلی قرار دارد. مجموعه داده اصلی را می توان به خوبی فقط با یک عدد نمایش داد - یعنی موقعیت هر داده روی جهت ویژگی 1.5 - به جای دو عدد f1 \n",
    "                و f2.\n",
    "            </p>\n",
    "        </div>\n",
    "        <div align=\"center\">\n",
    "            <img src=\"images/1.png\" alt=\"توضیح عکس\" width=\"600\">\n",
    "            شکل 6-1\n",
    "        </div>\n",
    "        <br>\n",
    "        <p style=\"text-align: justify;\">\n",
    "            ایده ی کلیدی در اینجا آن است که ویژگی‌های تکراری را با چند ویژگی جدید جایگزین کنیم که اطلاعات موجود در فضای ویژگی‌های اصلی را به‌طور کافی خلاصه کنند. زمانی که فقط دو ویژگی وجود دارد، به‌راحتی می‌توان تشخیص داد که ویژگی جدید باید چگونه باشد. اما این کار زمانی که فضای ویژگی‌های اصلی دارای صدها یا هزاران بُعد باشد، بسیار دشوار می‌شود. ما نیاز داریم که به‌صورت ریاضی ویژگی‌های جدیدی را که به دنبالشان هستیم، توصیف کنیم. سپس می‌توانیم از تکنیک‌های بهینه‌سازی برای یافتن آن‌ها استفاده کنیم.\n",
    "        یکی از راه‌های تعریف ریاضی برای عبارت «خلاصه‌سازی مناسب اطلاعات» این است که بگوییم تودهٔ داده‌های جدید باید تا حد امکان حجم (volume) فضای اصلی را حفظ کند. ما در حال فشرده‌سازی تودهٔ داده‌ها به یک پنکیک تخت هستیم، اما می‌خواهیم این پنکیک در جهت‌های درست، بیشترین اندازه را داشته باشد. این بدان معناست که باید راهی برای اندازه‌گیری حجم داشته باشیم.\n",
    "            حجم با فاصله مرتبط است. اما مفهوم فاصله در یک توده از نقاط داده، کمی مبهم است. می‌توان فاصلهٔ بیشینه بین هر دو نقطه را اندازه‌گیری کرد، اما این کار از نظر ریاضی تابعی بسیار دشوار برای بهینه‌سازی محسوب می‌شود.\n",
    "یک جایگزین، اندازه‌گیری میانگین فاصله بین زوج‌های نقاط است — یا به‌طور معادل، میانگین فاصلهٔ هر نقطه از میانگین کل نقاط — که همان واریانس (variance) است.\n",
    "        این روش از نظر محاسباتی بسیار آسان‌تر برای بهینه‌سازی است. (زندگی دشوار است. آمارشناس ها به راه‌های ساده‌تر عادت کرده‌اند.)\n",
    "        </p>\n",
    "        <hr><br>\n",
    "        <div>\n",
    "            <h3>تعریف نماد های ریاضی</h3>\n",
    "            <hr>\n",
    "            <p style=\"text-align: justify;\">\n",
    "                X : نشان دهنده ماتریس داده با ابعاد n*d است که n تعداد نمونه ها و d تعداد ویژگی ها می باشد.<br>\n",
    "                x : یک بردار ستونی شامل یک نقطه داده ( ترانهاده یک سطر از ماتریس داده ).<br>\n",
    "                v : نشان دهنده یکی از بردار های ویژگی جدید یا همان مولفه های اصلی که سعی در پیداکردن آن داریم.\n",
    "            </p>\n",
    "        </div>\n",
    "        <hr>\n",
    "        <br>\n",
    "        <div>\n",
    "            <h3>نگاشت خطی</h3>\n",
    "            <hr>\n",
    "            <p style=\"text-align: justify;\">\n",
    "                در این بخش مراحل PCA را گام به گام بررسی می کنیم. شکل 6-2 کل فرآیند را نشان میدهد.\n",
    "            </p>\n",
    "            <div align=\"center\">\n",
    "                <img src=\"images/2.png\" alt=\"توضیح عکس\" width=\"800\">\n",
    "                <p style=\"text-align: center;\">شکل 6-2 نمایش PCA : (الف) داده های اصلی در فضای ویژگی\n",
    "                    (ب) داده های متمرکز\n",
    "                    (ج) تصویر کردن یک بردار داده x بر روی بردار v\n",
    "                    (د) جهت حداکثر واریانس مختصات نگاشت شده \n",
    "                </p>\n",
    "                <p style=\"text-align: justify;\">\n",
    "                    PCA از نگاشت خطی برای تبدیل داده‌ها به فضای ویژگی جدید استفاده می‌کند.\n",
    "‏شکل ‎۶-۲(c)‎ نشان می‌دهد که یک نگاشت خطی چگونه به نظر می‌رسد. زمانی که ‎x‎ را روی ‎v‎ نگاشت می‌کنیم، طول نگاشت متناسب با ضرب داخلی بین این دو است، نرمال‌سازی شده بر اساس نرم ‎v‎ (یعنی ضرب داخلی ‎v‎ با خودش).\n",
    "‏در مراحل بعدی، ‎v‎ را به داشتن نرم واحد محدود خواهیم کرد. بنابراین، تنها بخش مرتبط، صورت کسر است — بیایید آن را ‎z‎ بنامیم.\n",
    "<br>\n",
    "<span style=\"font-weight: bold;\">\n",
    "                        ‏معادله ۶-۱. مختصات نگاشت\n",
    "    <span>\n",
    "        \\[ Z = x^TV \\]\n",
    "    </span>\n",
    "</span>\n",
    "                    <br>\n",
    "‏توجه داشته باشید که ‎z‎ یک اسکالر است، در حالی که ‎x‎ و ‎v‎ بردارهای ستونی هستند.\n",
    "‏از آنجا که تعداد زیادی نقطه داده وجود دارد، می‌توانیم بردار ‎z‎ از تمام مختصات نگاشت‌شده آن‌ها روی ویژگی جدید ‎v‎ را فرمول‌بندی کنیم (معادله ‎۶-۲‎).\n",
    "‏در این‌جا، ‎X‎ ماتریس داده‌ای آشنا است که هر ردیف آن یک نقطه داده است.\n",
    "‏z حاصل، یک بردار ستونی خواهد بود.\n",
    "<br>\n",
    "<span style=\"font-weight: bold;\">      \n",
    "    ‏معادله ۶-۲. بردار مختصات نگاشت‌شده\n",
    "    <span>\n",
    "        \\[ Z = XV \\]\n",
    "    </span>\n",
    "</span>\n",
    "            </p>\n",
    "            </div>\n",
    "        </div>\n",
    "        <br>\n",
    "        <div>\n",
    "            <h3>واریانس و واریانس تجربی</h3>\n",
    "        </div>\n",
    "        <hr>\n",
    "        <p style=\"text-align: justify;\">گام بعدی، محاسبه‌ی واریانس نگاشت‌هاست. واریانس به‌عنوان امید ریاضی فاصله‌ی مربعی تا میانگین تعریف می‌شود (معادله ۶-۳).<br>\n",
    "            <br><span style=\"font-weight: bold;\">معادله ۶-۳. واریانس یک متغیر تصادفی Z</span><br>\n",
    "            <span>Var(Z) = E[(Z – E(Z))2]</span> <br>\n",
    "            اما در این صورت بندی مساله یک مشکل کوچک وجود دارد. صورت بندی ما از مساله چیزی درباره میانگین E(Z) نمی گوید.\n",
    "            و این متغیر یک متغیر آزاد می باشد. <br>\n",
    "            یکی از راه حل ها این است که آن را از میانگین حذف کنیم و این کار نیز با کم کردن میانگین از هر نقطه داده ممکن است  در این حالت داده به دست آمده دارای \n",
    "            میانگین صفر است و این به این معنی است که واریانس صرفا برابر با امید ریاضی Z^2 خواهد بود.\n",
    "            از دیدگاه هندسی، کم کردن میانگین معادل با مرکز‌سازی داده‌هاست. به شکل‌های ۶-۲(a-b) مراجعه کنید.\n",
    "            <br>\n",
    "            کمیتی که ارتباط نزدیکی با واریانس دارد، کوواریانس بین دو متغیر تصادفی Z1 و Z2 است\n",
    "            (معادله 6-4).\n",
    "            می‌توانید آن را به‌عنوان تعمیم مفهوم واریانس (برای یک متغیر تصادفی) به دو متغیر تصادفی در نظر بگیرید.<br>\n",
    "            <span style=\"font-weight: bold;\">معادله ۶-۴. کوواریانس بین دو متغیر تصادفی Z1 , Z2</span><br>\n",
    "            <span>Cov(Z1, Z2) = E[(Z1 – E(Z1))(Z2 – E(Z2))]</span><br>\n",
    "            وقتی متغیرهای تصادفی میانگین صفر داشته باشند، کوواریانس آن‌ها با هم‌بستگی خطی‌شان برابر است، یعنی E[Z1Z2]. در ادامه بیشتر به این مفهوم خواهیم پرداخت.<br>\n",
    "            مقادیر آماری مانند واریانس و امید ریاضی، بر پایه‌ی توزیع داده‌ها تعریف می‌شوند.\n",
    "            اما در عمل، ما به توزیع واقعی\n",
    "            داده‌ها دسترسی\n",
    "            نداریم، بلکه\n",
    "            فقط مجموعه‌ای از داده‌های مشاهده‌شده داریم:\n",
    "            z1,...,zn\n",
    "            این حالت را توزیع تجربی (empirical distribution) می‌نامند، و با استفاده از آن می‌توانیم یک تخمین تجربی از واریانس به دست آوریم (معادله ۶-۵).<br>\n",
    "            <span>معادله ۶-۵. واریانس تجربی Z بر پایه مشاهدات z </span><br>\n",
    "            <span>$$\n",
    "\\text{Var}_{\\text{emp}}(Z) = \\frac{1}{n - 1} \\sum_{i=1}^{n} z_i^2\n",
    "$$</span><br>\n",
    "            در اینجا فرض شده که داده‌ها از پیش مرکز‌سازی شده‌اند (میانگین صفر دارند)، بنابراین نیازی به کم کردن میانگین در فرمول نیست.\n",
    "        </p>\n",
    "        <hr><br>\n",
    "        <div>\n",
    "            <h3>مولفه های اصلی</h3><hr>\n",
    "            <ul>\n",
    "                <li>\n",
    "                    <h4>اولین صورت بندی</h4>\n",
    "                <p style=\"text-align: justify;\">\n",
    "                با استفاده از تعریف zi در معادله 6-1، می توانیم صورت بندی مساله بیشینه سازی واریانس داده های نگاشت شده را به دست آوریم که در معادله 6-6 نشان داده شده است.\n",
    "                در اینجا مخرج 1-n را از تعریف واریانس تجربی حذف کرده ایم زیرا آن یک ثابت جهانی است و در محل وقوع بیشنه تاثیری ندارد.\n",
    "                <br><span style=\"font-weight: bold;\">معادله ۶-۶. تابع هدف برای مؤلفه‌های اصلی</span><br>\n",
    "                <span>$$\n",
    "\\max_{w} \\sum_{i=1}^{n} (x_i^T w)^2, \\quad \\text{subject to} \\quad w^T w = 1\n",
    "$$</span>\n",
    "            محدودیت در اینجا باعث می‌شود که ضرب داخلی ( w) با خودش برابر با ۱ باشد، که معادل این است که بگوییم بردار باید طول واحد داشته باشد. این به این دلیل است که ما فقط به جهت ( w) اهمیت می‌دهیم و نه به اندازه آن. اندازه ( w) یک درجه آزادی غیرضروری است، بنابراین با تنظیم آن به یک مقدار دلخواه، این افزونگی را حذف می‌کنیم.\n",
    "            </p>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <h4>صورت بندی ماتریس-بردار</h4>\n",
    "                    <p style=\"text-align: justify;\">\n",
    "                        مرحله بعدی، مرحله‌ای پیچیده است. عبارت جمع مجذورها در معادله 6-6 کمی دست‌وپاگیر است. آیا می‌توان آن را به‌صورت ماتریس-بردار به شکلی تمیزتر بیان کرد؟ پاسخ مثبت است. کلید در هویت جمع مجذورها نهفته است: جمع تعدادی از عبارات به توان دو برابر است با توان دوم نرم بردارهایی که عناصر آن‌ها همان عبارات هستند، که این خود معادل ضرب داخلی بردار با خودش است. با استفاده از این هویت، می‌توانیم معادله 6-6 را به‌صورت نمادگذاری ماتریس-بردار بازنویسی کنیم، همان‌طور که در معادله 6-7 نشان داده شده است.\n",
    "                        <br><span style=\"font-weight: bold;\">معادله ۶-۷. تابع هدف مؤلفه‌های اصلی در صورت‌بندی ماتریسی-برداری</span><br>\n",
    "                        <span>\n",
    "                            $$\n",
    "\\max_{w} w^T X^T X w, \\quad \\text{subject to} \\quad w^T w = 1\n",
    "$$\n",
    "                        </span>\n",
    "                        این فرمول‌بندی PCA هدف را واضح‌تر بیان می‌کند: ما به دنبال جهتی در ورودی هستیم که نرم خروجی را بیشینه کند. آیا این آشنا به نظر می‌رسد؟ پاسخ در تجزیه مقدار منفرد (SVD) ماتریس \\( X \\) نهفته است. همان‌طور که مشخص می‌شود، \\( w \\) بهینه، بردار منفرد چپ اصلی ماتریس \\( X \\) است، که همچنین بردار ویژه اصلی ماتریس \\( X^T X \\) است. داده‌های تصویرشده به عنوان یک مؤلفه اصلی داده‌های اولیه شناخته می‌شوند.\n",
    "                    </p>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <h4>راه‌حل عمومی برای مؤلفه‌های اصلی</h4>\n",
    "                    <p style=\"text-align: justify;\">\n",
    "                     فرآیند بالا را می‌توان تکرار کرد. پس از یافتن اولین مؤلفه اصلی، می‌توانیم معادله ۶-۷ را دوباره اجرا کنیم با این شرط اضافی که بردار جدید بر بردارهایی که قبلاً یافت‌شده اند عمود باشد(به معادله ۶-۸ مراجعه کنید).\n",
    "                        <br><br><span style=\"font-weight: bold;\">معادله ۶-۸: تابع هدف برای مؤلفه اصلی \\( k+1 \\)-ام</span>\n",
    "                        <span>$$\n",
    "\\max_{w} w^T X^T X w, \\quad \\text{subject to} \\quad w^T w = 1, \\quad w^T w_i = 0, \\quad \\forall i = 1, \\dots, k\n",
    "$$</span>\n",
    "                        راه‌حل، \\( k+1 \\)-ام بردارهای منفرد سمت چپ ماتریس \\( X \\) است که بر اساس مقادیر منفرد نزولی مرتب شده‌اند. بنابراین، \\( k \\) مؤلفه اصلی اول، متناظر با \\( k \\) بردار منفرد سمت چپ اول ماتریس \\( X \\) هستند.\n",
    "                    </p>\n",
    "                </li>\n",
    "            </ul>\n",
    "        </div>\n",
    "        <hr><br>\n",
    "        <div>\n",
    "            <h3>تبدیل ویژگی ها</h3><hr>\n",
    "            <p style=\"text-align: justify;\">\n",
    "پس از یافتن مؤلفه‌های اصلی، می‌توان ویژگی‌ها را با استفاده از نگاشت خطی تبدیل کرد. فرض کنید \\( X = U \\Sigma V^T \\) تجزیه مقادیر منفرد (SVD) ماتریس \\( X \\) باشد و \\( V_k \\) ماتریسی باشد که ستون‌های آن شامل \\( k \\) بردار منفرد سمت چپ اول است. ماتریس \\( X \\) دارای ابعاد \\( n \\times d \\) است، که در آن \\( d \\) تعداد ویژگی‌های اصلی است، و \\( V_k \\) دارای ابعاد \\( d \\times k \\) است. به جای استفاده از یک بردار نگاشت واحد مانند معادله ۶-۲، می‌توان به طور همزمان روی چندین بردار در یک ماتریس نگاشت کرد (معادله ۶-۹)\n",
    "            </p>\n",
    "            <br><span style=\"font-weight: bold;\">معادله ۶-۹: ماتریس نگاشت PCA</span><br>\n",
    "            <span>\\[W = V_k\\]</span>\n",
    "            <p style=\"text-align: justify;\">\n",
    "                ماتریس مختصات نگاشت شده به راحتی قابل محاسبه است و با استفاده از این واقعیت که بردارهای منفرد نسبت به یکدیگر عمود هستند، می‌توان آن را ساده‌تر کرد (به معادله ۶-۱۰ مراجعه کنید).\n",
    "            </p>\n",
    "            <span style=\"font-weight: bold;\">معادله ۶-۱۰: تبدیل ساده PCA</span>\n",
    "            <span>\\[Z = X W = X V_k = U \\Sigma V^T V_k = U_k \\Sigma_k\\]</span>\n",
    "            <p style=\"text-align: justify;\">\n",
    "                مقادیر پیش‌بینی‌شده در واقع بردارهای راست منفرد اول تا 𝑘\n",
    "ام هستند که در مقادیر منفرد اول تا 𝑘\n",
    "ام ضرب شده‌اند. بنابراین، کل راه‌حل PCA، شامل مؤلفه‌ها و نگاشت ها ، می‌تواند به‌طور مناسب از طریق تجزیه مقدار منفرد (SVD) ماتریس \n",
    "X به‌دست آید.\n",
    "            </p>\n",
    "        </div><hr><br>\n",
    "        <div>\n",
    "            <h3>پیاده سازی PCA</h3><hr>\n",
    "            <p style=\"text-align: justify;\">\n",
    "                در بسیاری از منابع، برای انجام PCA ابتدا داده‌ها را مرکز‌سازی می‌کنند (میانگین را از هر بُعد کم می‌کنند) و سپس تجزیه به مؤلفه‌های اصلی را با استفاده از ماتریس کوواریانس و محاسبه بردارها و مقادیر ویژه انجام می‌دهند.\n",
    "اما ساده‌ترین و کارآمدترین روش برای پیاده‌سازی PCA، استفاده از تجزیه مقدار منفرد یا همان SVD است که روی ماتریس داده‌های مرکز‌شده انجام می‌شود.\n",
    "            </p\n",
    "            <h4>مراحل اجرای PCA:</h4>\n",
    "            <ol>\n",
    "                <li>مرکز‌سازی داده‌ها:\n",
    "                    <br>\n",
    "                    ابتدا باید داده‌ها را طوری تغییر دهیم که میانگین هر ویژگی (ستون) برابر صفر شود. این کار با استفاده از فرمول زیر انجام می‌شود:\n",
    "                    <span>\n",
    "                        \\[C = X - \\mathbf{1} \\mu^T\\]\n",
    "                    </span>\n",
    "                        در اینجا: \n",
    "                    <br>\n",
    "                    \\( X \\) ماتریس اصلی داده ها است (ردیف ها نمونه ها و ستون ها ویژگی ها هستند).<br>\n",
    "                    \\( \\mu \\) یک بردارستونی است که میانگین هر ستون از \\( X \\) را نشان میدهد.<br>\n",
    "1 برداری ستونی است که تمام عناصرش برابر 1 هستند و به اندازه تعداد نمونه‌ها طول دارد\n",
    "نتیجه، ماتریس \n",
    "\\( C \\)\n",
    "است که داده‌ها را با میانگین صفر در هر ستون نشان می‌دهد.\n",
    "                </li><br>\n",
    "                <li>\n",
    "                    محاسبه SVD:<br>\n",
    "                    روی ماتریس \\( C \\) تچزیه مقدار منفرد (SVD) انجام میدهیم:<br>\n",
    "                    <span>\n",
    "                        \\[ X = U  \\Sigma V^T \\]\n",
    "                    </span>\n",
    "                    که در آن:<br>\n",
    "                    \\( U \\) ماتریسی است که بردار های ویژه سمت چپ را دارد.<br>\n",
    "                    \\( \\Sigma \\) ماتریسی قطری شامل مقادیر منفرد (مشابه با جذر مقادیر ویژه) است.<br>\n",
    "                    \\( V \\) ماتریسی است که بردارهای ویژه سمت راست (یا همان مؤلفه‌های اصلی) را دارد.\n",
    "                </li><br>\n",
    "                <li>\n",
    "                    استخراج مولفه های اصلی:<br>\n",
    "                    برای به‌دست‌آوردن مؤلفه‌های اصلی، فقط کافی‌ست چند ستون اول از ماتریس \n",
    "𝑉\n",
    "را برداریم.\n",
    "به‌طور مشخص، اولین \n",
    "𝑘\n",
    "مؤلفه اصلی، \n",
    "𝑘\n",
    "ستون اول ماتریس \n",
    "𝑉\n",
    "هستند. این مؤلفه‌ها جهاتی از فضا هستند که بیشترین واریانس داده در آن‌ها قرار دارد.\n",
    "                </li><br>\n",
    "                <li>\n",
    "                    تبدیل داده ها به فضای جدید: <br>\n",
    "                    برای تبدیل داده‌ها به فضای جدید (یعنی نمایش داده‌ها بر اساس مؤلفه‌های اصلی)، کافی است اولین \\( k \\)\n",
    "                    ستون ماتریس \\( U \\)\n",
    "                    را برداریم.<br>\n",
    "                    در واقع، این کار داده‌ها را در فضای جدید فشرده می‌کند و اطلاعات اصلی را حفظ می‌کند.<br>\n",
    "                    اگر بخواهیم \"سفید سازی\" (whitening) انجام دهیم \n",
    "                    یعنی مؤلفه‌ها هم مستقل و هم با واریانس یک باشند باید بردار های \\( U \\) را با معکوس مقادیر منفرد متناظر در \\( \\Sigma \\) \n",
    "                    مقیاس دهی کنیم.\n",
    "                </li><br>\n",
    "            </ol>\n",
    "            <h4>PCA در عمل: تحلیل تصاویر MNIST</h4>\n",
    "              <div>\n",
    "                <h4>درباره مجموعه داده</h4>\n",
    "                <ul>\n",
    "                  <li>مجموعه داده MNIST شامل تصاویر دست‌نویس ارقام ۰ تا ۹ است.</li>\n",
    "                  <li>تصاویر اصلی دارای اندازه ۲۸ × ۲۸ پیکسل هستند (۷۸۴ ویژگی).</li>\n",
    "                  <li>نسخه ساده‌ شده در <code>scikit-learn</code> به ۸ × ۸ پیکسل کاهش یافته است (۶۴ ویژگی).</li>\n",
    "                </ul>\n",
    "              </div>\n",
    "              <div>\n",
    "                <h4>مراحل انجام کار</h4>\n",
    "                <ol>\n",
    "                  <li>اجرای الگوریتم PCA روی داده‌ها</li>\n",
    "                  <li>کاهش ابعاد به ۳ مؤلفه اصلی</li>\n",
    "                  <li>نمایش داده‌ها در فضای سه‌بعدی با استفاده از این مؤلفه‌ها</li>\n",
    "                </ol>\n",
    "              </div>\n",
    "              <div>\n",
    "                <h4>هدف</h4>\n",
    "                <ul>\n",
    "                  <li>درک بهتر ساختار پنهان در داده‌ها</li>\n",
    "                  <li>ساده‌سازی نمایش با کاهش ابعاد</li>\n",
    "                  <li>آشکارسازی الگوها و تفاوت‌های بین ارقام</li>\n",
    "                </ul>\n",
    "              </div>\n",
    "        </div>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b037ee-bfa7-488f-8ec2-551219af1d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# برای جلوگیری از خطا در رسم نمودار، matplotlib را به صورت inline تنظیم می‌کنیم\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the digits dataset\n",
    "digits_data = datasets.load_digits()\n",
    "n = len(digits_data.images)\n",
    "\n",
    "# Flatten 8x8 image data into 1D vectors (64-dimensional)\n",
    "image_data = digits_data.images.reshape((n, -1))\n",
    "print(\"Shape of image_data:\", image_data.shape)\n",
    "\n",
    "# Ground truth labels\n",
    "labels = digits_data.target\n",
    "print(\"Labels:\", labels)\n",
    "\n",
    "# Fit PCA to retain at least 80% of the total variance\n",
    "pca_transformer = PCA(n_components=0.8)\n",
    "pca_images = pca_transformer.fit_transform(image_data)\n",
    "\n",
    "# Explained variance ratio for each component\n",
    "print(\"Explained variance ratio:\", pca_transformer.explained_variance_ratio_)\n",
    "\n",
    "# Sum of the first 3 principal components' variance ratio\n",
    "print(\"Sum of first 3 explained variance ratios:\",\n",
    "      pca_transformer.explained_variance_ratio_[:3].sum())\n",
    "\n",
    "# Visualize the first 3 principal components in a 3D scatter plot\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for i in range(100):\n",
    "    ax.scatter(pca_images[i, 0], pca_images[i, 1], pca_images[i, 2],\n",
    "               marker=r'${}$'.format(labels[i]), s=64)\n",
    "\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')\n",
    "ax.set_zlabel('Principal Component 3')\n",
    "plt.title(\"PCA of Digits Dataset (First 3 Components)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceb84ec-74aa-4d93-86db-7665ac12f25f",
   "metadata": {},
   "source": [
    "<div style=\"direction: rtl; line-height: 300%;\">\n",
    "    <br>\n",
    "    <font face=\"XB Zar\" size=\"5\">\n",
    "        <p style=\"text-align: justify;\">\n",
    "            از آن‌جا که بین ارقام (اعداد دست‌نویس) هم‌پوشانی قابل توجهی وجود دارد، تشخیص آن‌ها از یکدیگر در فضای تصویرشده (فضایی که داده‌ها به کمک PCA به آن نگاشته شده‌اند) برای یک دسته‌بند خطی دشوار خواهد بود.\n",
    "به عبارت دیگر، زمانی که داده‌های اصلی ۶۴ بُعدی را فقط به سه مؤلفه اصلی کاهش می‌دهیم (یعنی آن‌ها را در فضای سه‌بعدی نمایش می‌دهیم)، داده‌ها هنوز آن‌قدر با هم تداخل دارند که یک مدل خطی نمی‌تواند به خوبی بین آن‌ها مرز تصمیم‌گیری رسم کند.\n",
    "بنابراین، اگر هدف ما طبقه‌بندی ارقام دست‌نویس باشد و بخواهیم از یک مدل خطی (مثل رگرسیون لجستیک یا SVM خطی) برای این کار استفاده کنیم، فقط سه مؤلفه‌ی اصلی برای رسیدن به دقت مطلوب کافی نیستند.\n",
    "با این حال، نکته‌ی جالب اینجاست که حتی با فقط سه مؤلفه، می‌توان بخش قابل توجهی از اطلاعات موجود در فضای ۶۴ بُعدی اولیه را حفظ کرد. یعنی کاهش ابعاد به سه بُعد، بخش زیادی از ساختار کلی داده‌ها را نگه می‌دارد، هرچند برای طبقه‌بندی دقیق کافی نیست.\n",
    "        </p>\n",
    "        <hr><br>\n",
    "        <div>\n",
    "            <h3>سفید سازی و ZCA</h3><hr>\n",
    "            <p style=\"text-align: justify;\">\n",
    "                به‌خاطر وجود قید تعامد (orthogonality) در تابع هدف PCA، تبدیل حاصل از آن یک ویژگی مفید ایجاد می‌کند:\n",
    "ویژگی‌های (features) تبدیل‌شده دیگر با هم همبستگی ندارند.\n",
    "به بیان دیگر، ضرب داخلی بین هر دو بردار ویژگی برابر صفر است. این یعنی بردارهای جدید حاصل از PCA نسبت به هم کاملاً عمود هستند و مستقل از هم رفتار می‌کنند.\n",
    "اثبات این موضوع با استفاده از خاصیت تعامد بردارهای منفرد (singular vectors) ساده است:\n",
    "                <span>\n",
    "                    \\[ Z^TZ = \\Sigma_k U_k^TU_k\\Sigma_k = \\Sigma_k^2 \\]\n",
    "                </span>\n",
    "                در نتیجه ما یک ماتریس قطری داریم که عناصر آن مربعات مقادیر تکین می باشد و این مقدار نشان دهنده همبستگی هر بردار ویژگی با خودش است .\n",
    "                در بعضی مواقع نرمال سازی مقیاس ویژگی ها به 1 مفید است که به این کار در حوزه پردازش سیگنال سفید سازی می گویند. \n",
    "                این کار از نظر ریاضی منجر به یک مجموعه ای از ویژگی ها میشود که مقدار همبستگی واحدی با خودشان دارند و همبستگی شان با بقیه ویژگی ها 0 است .\n",
    "                همان طور که مشخص است می توان سفید سازی را با ضرب تبدیل PCA در معکوس مقادیر منفرد انجام داد \n",
    "            </p>\n",
    "            <p style=\"text-align: justify;\">\n",
    "                معادله 6-11 : PCA به همراه سفید سازی \n",
    "                <br>\n",
    "                <span>\n",
    "                    \\[ W_{\\text{white}} = V_k \\Sigma_k^{-1} \\]\n",
    "                </span>\n",
    "                <span>\n",
    "                    \\[ Z_{\\text{white}} = X V_k \\Sigma_k^{-1} = U \\Sigma V^T V_k \\Sigma_k^{-1} = U_k \\]\n",
    "                </span>\n",
    "                سفید سازی کاملا مستقل از کاهش ابعاد است و میتوان هر کدام را به صورت مستقل از دیگری انجام داد . \n",
    "                به طور مثال ZCA یک سفید سازی است که خیلی شبیه به PCA است اما هیچ کاهش بعدی انجام نمیدهد .\n",
    "                سفید سازی ZCA از مجموعه کامل مولفه های اصلی V بدون هیچ کاهش بعد و ویژگی استفاده میکند و یک ضرب اضافی ترانهاده V  دارد .\n",
    "                <br>\n",
    "                معادله 6-12 : سفیدسازی ZCA\n",
    "                <br>\n",
    "                <span>\n",
    "\\[\n",
    "W_{\\text{ZCA}} = V \\Sigma^{-1} V^T\n",
    "\\]\n",
    "\\[\n",
    "Z_{\\text{ZCA}} = X V \\Sigma^{-1} V^T = U \\Sigma V^T V \\Sigma^{-1} V^T = U\n",
    "\\]\n",
    "                </span>\n",
    "                نگاشت PCA داده ها را به یک فضای ویژگی جدید می برد که ویژگی ها  به عنوان پایه عمل میکنند اما ZCA\n",
    "                با ضرب اضافی که دارد داده هایی تولید می کند که از نظر اقلیدسی خیلی شبیه به داده های اصلی است و داده ها به فضایی شبیه به فضای اصلی نگاشت می شوند . \n",
    "            </p>\n",
    "        </div>\n",
    "        <hr><br>\n",
    "        <div>\n",
    "            <h3>ملاحظات و محدودیت های PCA</h3><hr>\n",
    "    <h3>خلاصه نکات درباره PCA و کاهش ابعاد</h3>\n",
    "    <h3>انتخاب تعداد مؤلفه‌ها (k)</h3>\n",
    "    <ul>\n",
    "        <li>تعداد مؤلفه‌ها (k) یک ابرپارامتر است و باید به‌گونه‌ای انتخاب شود که کیفیت مدل حفظ شود.</li>\n",
    "        <li>روش رایج: انتخاب k طوری که درصد مشخصی از واریانس کل داده‌ها حفظ شود.</li>\n",
    "        <li>واریانس مؤلفه kام برابر است با:\n",
    "            <span>\\[ \\| X v_k \\|^2 = \\| u_k \\sigma_k \\|^2 = \\sigma_k^2 \\]</span>\n",
    "        </li>\n",
    "    </ul>\n",
    "    <h3>تحلیل طیف (Spectral Analysis)</h3>\n",
    "    <ul>\n",
    "        <li>طیف (spectrum) لیستی مرتب از مقادیر منفرد ماتریس است.</li>\n",
    "        <li>اگر فقط چند مقدار بزرگ و بقیه کوچک باشند، می‌توان فقط بزرگ‌ترین‌ها را حفظ کرد.</li>\n",
    "        <li>اگر بین مقدارهای بزرگ و کوچک شکاف واضحی وجود دارد، همانجا نقطه برش k مناسبی است.</li>\n",
    "        <li>نیاز به بررسی چشمی دارد و مناسب پایپ‌لاین خودکار نیست.</li>\n",
    "    </ul>\n",
    "    <h3>محدودیت‌های PCA</h3>\n",
    "    <h4>1. تفسیرپذیری پایین</h4>\n",
    "    <ul>\n",
    "        <li>مؤلفه‌ها ترکیب‌های خطی از ویژگی‌ها هستند و مقادیر تصویر ممکن است منفی یا مثبت باشند.</li>\n",
    "        <li>در کاربردهایی مثل بازار سهام، عوامل به‌دست‌آمده تفسیر انسانی ندارند.</li>\n",
    "        <li>در نتیجه اعتماد به خروجی مدل مشکل است.</li>\n",
    "    </ul>\n",
    "    <h4>2. هزینه محاسباتی بالا</h4>\n",
    "    <ul>\n",
    "        <li>PCA مبتنی بر تجزیه مقدار منفرد (SVD) است که هزینه‌بر است.</li>\n",
    "        <li>پیچیدگی SVD کامل:\n",
    "            <div class=\"math\">O(n d² + d³) &nbsp;&nbsp;&nbsp; (برای n ≥ d)</div>\n",
    "        </li>\n",
    "        <li>پیچیدگی SVD کاهش‌یافته برای k مؤلفه:\n",
    "            <div class=\"math\">O((n + d)² k) ≈ O(n² k)</div>\n",
    "        </li>\n",
    "    </ul>\n",
    "    <h4>3. سختی در اجرای جریانی (Streaming)</h4>\n",
    "    <ul>\n",
    "        <li>اجرای PCA به صورت مرحله‌ای یا بر نمونه‌ای از کل داده مشکل است.</li>\n",
    "        <li>الگوریتم‌هایی برای این کار وجود دارند ولی دقت کمتری دارند.</li>\n",
    "        <li>تغییر در توزیع داده‌ها نیازمند بازمحاسبه PCA است.</li>\n",
    "    </ul>\n",
    "    <h4>4. حساسیت به داده‌های خام و پرت</h4>\n",
    "    <ul>\n",
    "        <li>استفاده از داده‌های خام شمارشی (تعداد کلمات، مشاهده فیلم و...) مناسب نیست.</li>\n",
    "        <li>وجود مقادیر پرت می‌تواند واریانس و همبستگی را به‌شدت منحرف کند.</li>\n",
    "        <li>پیشنهاد می‌شود:\n",
    "            <ul>\n",
    "                <li>مقادیر پرت حذف شوند (Frequency-Based Filtering)</li>\n",
    "                <li>تبدیل‌هایی مانند tf-idf یا log transformation استفاده شوند</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "        </div><hr><br>\n",
    "        <div>\n",
    "            <h3>کاربردها</h3><hr>\n",
    "    <p style=\"text-align: justify;\">\n",
    "        PCA ابعاد فضای ویژگی‌ها را با پیدا کردن الگوهای خطی که بین ویژگی‌ها وجود دارد کاهش می‌دهد. \n",
    "        چون این روش بر پایه تجزیه مقدار منفرد (SVD) است، وقتی تعداد ویژگی‌ها خیلی زیاد باشد، محاسبات آن سنگین می‌شود. \n",
    "        اما برای مجموعه‌های کوچکتر که ویژگی‌های عددی دارند، استفاده از PCA بسیار ارزشمند است.\n",
    "    </p>\n",
    "    <p style=\"text-align: justify;\">\n",
    "        وقتی از PCA استفاده می‌کنیم، بخشی از اطلاعات داده‌ها حذف می‌شود. \n",
    "        بنابراین مدل‌هایی که بعد از آن ساخته می‌شوند ممکن است سریع‌تر آموزش ببینند ولی دقتشان کمتر شود. \n",
    "        برای مثال در مجموعه داده معروف MNIST، دیده شده که کاهش ابعاد با PCA گاهی باعث کاهش دقت مدل‌های طبقه‌بندی می‌شود. \n",
    "        پس PCA مزایا و معایب خودش را دارد.\n",
    "    </p>\n",
    "    <p style=\"text-align: justify;\">\n",
    "        یکی از کاربردهای جالب PCA، تشخیص ناهنجاری‌ها در داده‌های سری زمانی است. \n",
    "        در تحقیقی که توسط لاکینا و همکارانش در سال 2004 انجام شد، از PCA برای پیدا کردن مشکلات و ناهنجاری‌ها در ترافیک اینترنت استفاده کردند. \n",
    "        آنها تمرکز خود را روی تغییرات ناگهانی حجم ترافیک گذاشتند، یعنی مواقعی که ناگهان ترافیک بین دو منطقه شبکه زیاد یا کم می‌شود. \n",
    "        این اتفاق‌ها می‌توانند نشانه مشکلات شبکه یا حملات انکار سرویس باشند. \n",
    "        بنابراین دانستن زمان و مکان این تغییرات برای مدیریت شبکه اهمیت زیادی دارد.\n",
    "    </p>\n",
    "    <p style=\"text-align: justify;\">\n",
    "        چون حجم کلی ترافیک اینترنت بسیار زیاد است، تشخیص افزایش ناگهانی ترافیک در بخش‌های کوچک سخت است. \n",
    "        تعداد کمی از لینک‌های اصلی حجم زیادی از ترافیک را عبور می‌دهند. نکته این است که این تغییرات معمولاً روی چند لینک به طور همزمان تاثیر می‌گذارند چون بسته‌های داده باید از چند گره عبور کنند. \n",
    "        اگر هر لینک را یک ویژگی در نظر بگیریم و میزان ترافیک هر لینک را در بازه‌های زمانی مختلف اندازه بگیریم، داده‌ها به شکل ماتریسی درمی‌آیند که مؤلفه‌های اصلی آن روند کلی ترافیک را نشان می‌دهند و بقیه مؤلفه‌ها مربوط به ناهنجاری‌ها هستند.\n",
    "    </p>\n",
    "    <p style=\"text-align: justify;\">\n",
    "        PCA در مدل‌سازی مالی هم کاربرد دارد. اینجا PCA به شکل تحلیل عاملی استفاده می‌شود؛ یعنی تلاش می‌کنیم تغییرات داده‌ها را با تعداد کمی عامل پنهان توضیح دهیم. در این موارد، هدف پیدا کردن خود مؤلفه‌ها است نه داده‌های تبدیل شده.\n",
    "    </p>\n",
    "    <p style=\"text-align: justify;\">\n",
    "        در بازار سهام، بازده سهام‌ها معمولاً با هم همبسته هستند؛ بعضی سهام‌ها با هم بالا و پایین می‌روند و بعضی دیگر برعکس. برای کاهش ریسک، یک پرتفوی خوب باید شامل سهام‌هایی باشد که زیاد با هم همبسته نیستند. \n",
    "        پیدا کردن این الگوهای همبستگی به تصمیم‌گیری سرمایه‌گذاران کمک می‌کند.\n",
    "    </p>\n",
    "    <p style=\"text-align: justify;\">\n",
    "        گاهی همبستگی سهام‌ها به صنایع مربوط می‌شود؛ مثلاً سهام شرکت‌های فناوری معمولاً با هم حرکت می‌کنند و سهام شرکت‌های هواپیمایی ممکن است وقتی قیمت نفت بالا می‌رود پایین بیایند. \n",
    "        اما گاهی همبستگی‌هایی پیدا می‌شود که قابل پیش‌بینی نیستند و تحلیل‌گران دنبال این الگوها هستند. \n",
    "        مدل‌های آماری مثل مدل عامل آماری، با استفاده از PCA، سهام‌هایی را که با هم نوسان دارند شناسایی می‌کنند. در این کاربرد هدف اصلی خود مؤلفه‌های اصلی است.\n",
    "    </p>\n",
    "    <p style=\"text-align: justify;\">\n",
    "        روش ZCA هم برای پردازش اولیه تصاویر مفید است. در تصاویر طبیعی، پیکسل‌های کناری معمولاً رنگ‌های مشابهی دارند و این باعث ایجاد همبستگی می‌شود. \n",
    "        سفیدسازی ZCA این همبستگی‌ها را حذف می‌کند تا مدل‌ها بهتر بتوانند ویژگی‌های مهم‌تر و ساختارهای جالب‌تر تصویر را یاد بگیرند. \n",
    "        پایان‌نامه کریژفسکی در سال 2009 مثال‌های خوبی درباره تاثیر ZCA روی تصاویر دارد.\n",
    "    </p>\n",
    "    <p style=\"text-align: justify;\">\n",
    "        بسیاری از مدل‌های یادگیری عمیق، PCA یا ZCA را به عنوان مرحله پیش‌پردازش استفاده می‌کنند، اگرچه همیشه لازم نیست. \n",
    "        مثلاً در مقاله‌ای از رانتزو و همکاران در 2010 گفته شده سفیدسازی لازم نیست ولی سرعت آموزش را افزایش می‌دهد. \n",
    "        در مقاله دیگری در 2011 اشاره شده که ZCA برای برخی مدل‌ها مفید است ولی برای همه خیر. \n",
    "        (این مدل‌ها معمولاً برای یادگیری ویژگی بدون نظارت هستند، پس ZCA به عنوان یکی از روش‌های مهندسی ویژگی در کنار روش‌های دیگر استفاده می‌شود. ترکیب روش‌ها در یادگیری ماشین معمول است.)\n",
    "    </p>\n",
    "        </div><hr><br>\n",
    "        <div>\n",
    "            <h3>خلاصه</h3><hr>\n",
    "             <p style=\"text-align: justify;\">\n",
    "        حالا که بحث PCA به پایان رسید، دو نکته کلیدی را همیشه به یاد داشته باشید: اول اینکه PCA داده‌ها را به صورت خطی روی فضایی جدید پروجکت می‌کند، و دوم اینکه هدفش این است که واریانس داده‌های تبدیل شده را حداکثر کند.\n",
    "    </p>\n",
    "    <p style=\"text-align: justify;\">\n",
    "        برای این کار، از تجزیه ویژه ماتریس کوواریانس استفاده می‌شود که رابطه نزدیکی با تجزیه مقدار منفرد (SVD) داده دارد. یک تصویر ذهنی ساده از PCA این است که داده‌ها را مثل یک پنکیک پهن و تا حد ممکن پفکی فشرده می‌کند.\n",
    "    </p>\n",
    "    <p style=\"text-align: justify;\">\n",
    "        PCA یکی از روش‌های معروف کاهش ابعاد است اما محدودیت‌هایی هم دارد، مانند هزینه محاسباتی بالا و سختی تفسیر نتایج. با این حال، وقتی بین ویژگی‌ها همبستگی خطی وجود دارد، PCA به عنوان یک مرحله پیش‌پردازش بسیار کاربردی است.\n",
    "    </p>\n",
    "    <p style=\"text-align: justify;\">\n",
    "        اگر PCA را روشی برای حذف همبستگی‌های خطی بدانیم، با مفهوم «سفیدسازی» (whitening) مرتبط می‌شود. روش مشابهی به نام ZCA وجود دارد که داده‌ها را به صورت قابل فهم‌تری سفید می‌کند ولی ابعاد داده‌ها را کاهش نمی‌دهد.\n",
    "    </p>\n",
    "        </div><hr><br>\n",
    "        <div>\n",
    "            <h3>منابع</h3><hr>\n",
    "            <p style=\"text-align: justify;\">\n",
    "                مرجع اصلی این جزوه کتاب <br>\n",
    "            Feature\n",
    "Engineering<br>\n",
    "for Machine Learning<br>\n",
    "PRINCIPLES AND TECHNIQUES FOR DATA SCIENTISTS<br>\n",
    "            می باشد . \n",
    "            </p>\n",
    "        </div>\n",
    "    </font>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ceeca3-78a5-4546-aa9d-d93f91b48b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
